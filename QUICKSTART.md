# üöÄ Guide de D√©marrage Rapide

Ce guide vous aidera √† d√©ployer votre mod√®le HessGPT sur Android en quelques √©tapes.

## üìã Pr√©requis

### 1. Environnement de d√©veloppement
- **Android Studio** Arctic Fox ou sup√©rieur
- **JDK 11+**
- **Python 3.8+** avec PyTorch 2.0+

### 2. V√©rifier votre installation Python
```bash
python --version  # Devrait afficher Python 3.8+
pip install torch torchvision  # Si pas d√©j√† install√©
```

## üî® √âtapes d'Installation

### √âtape 1: Pr√©parer votre mod√®le

#### Option A: Utiliser un mod√®le de d√©mo (pour tester)
```bash
cd python

# Cr√©er un tokenizer de d√©mo
python create_tokenizer.py --output ../app/src/main/assets/tokenizer.json

# Cr√©er un mod√®le de d√©mo (petit, pour tester l'app)
python export_model.py --checkpoint demo --output ../app/src/main/assets/model.ptl
```

#### Option B: Utiliser votre vrai mod√®le HessGPT
```bash
cd python

# 1. Exporter votre mod√®le entra√Æn√©
python export_model.py \
    --checkpoint /path/to/your/HessGPT/checkpoint.pt \
    --output ../app/src/main/assets/model.ptl \
    --max-seq-len 512

# 2. Quantifier pour optimiser (RECOMMAND√â)
python quantize_model.py \
    --input ../app/src/main/assets/model.ptl \
    --output ../app/src/main/assets/model_quantized.ptl \
    --compare

# 3. Utiliser le mod√®le quantifi√©
mv ../app/src/main/assets/model_quantized.ptl ../app/src/main/assets/model.ptl

# 4. Cr√©er le tokenizer
# Si vous avez un tokenizer HuggingFace:
python create_tokenizer.py \
    --from-hf /path/to/your/tokenizer \
    --output ../app/src/main/assets/tokenizer.json
```

### √âtape 2: V√©rifier que le mod√®le fonctionne
```bash
cd python

# Tester le mod√®le avant d√©ploiement
python test_model.py --model ../app/src/main/assets/model.ptl --inspect

# Vous devriez voir:
# ‚úÖ Tous les tests sont pass√©s!
# ‚ö° Latence moyenne: XXX ms
# üöÄ D√©bit: X.X tokens/sec
```

### √âtape 3: Adapter le code √† votre mod√®le

#### Modifier `export_model.py`

Ouvrez `python/export_model.py` et remplacez la classe `DemoGPT` par votre vraie architecture:

```python
# Importez votre mod√®le
sys.path.append("../HessGpt_RoPE")
from Core.model import HessGPT, ModelConfig

# Dans la fonction export_to_torchscript():
config = ModelConfig(
    vocab_size=50257,  # Votre taille de vocabulaire
    n_embd=768,        # Dimension d'embedding
    n_head=12,         # Nombre de t√™tes d'attention
    n_layer=12,        # Nombre de couches
    max_seq_len=max_seq_len,
    # Ajoutez vos param√®tres RoPE ici
)

model = HessGPT(config)
model.load_state_dict(checkpoint['model_state_dict'])
```

### √âtape 4: Build l'application Android

1. **Ouvrir le projet dans Android Studio**
   ```bash
   # Ouvrir Android Studio
   # File > Open > S√©lectionner HessGptMobileApp/
   ```

2. **Synchroniser Gradle**
   - Android Studio va automatiquement t√©l√©charger les d√©pendances
   - Attendez que la synchronisation se termine

3. **V√©rifier les assets**
   - Assurez-vous que `app/src/main/assets/` contient:
     - `model.ptl` (ou `model_quantized.ptl`)
     - `tokenizer.json`

4. **Build et Run**
   - Connectez un appareil Android (API 26+) ou lancez un √©mulateur
   - Cliquez sur le bouton "Run" (‚ñ∂Ô∏è) dans Android Studio
   - L'app va se compiler et s'installer sur votre appareil

### √âtape 5: Tester l'application

1. **Premier lancement**
   - L'app va charger le mod√®le (peut prendre 5-30 secondes selon la taille)
   - Vous verrez "Mod√®le charg√© ‚úì" quand c'est pr√™t

2. **Envoyer un message**
   - Tapez une question dans le champ de texte
   - Appuyez sur le bouton d'envoi
   - Le mod√®le va g√©n√©rer une r√©ponse en streaming

3. **Surveiller les performances**
   - La barre de statut affiche le d√©bit (tokens/sec)
   - V√©rifiez les logs dans Logcat pour plus de d√©tails

## üéØ Optimisations Recommand√©es

### 1. R√©duire la taille du mod√®le

Si votre mod√®le est trop gros (>200MB):

```bash
# Quantification dynamique INT8
python quantize_model.py --input model.ptl --output model_q8.ptl

# Si encore trop gros, r√©duisez la taille du mod√®le √† l'entra√Ænement:
# - Moins de couches (n_layer)
# - Dimension plus petite (n_embd)
# - Distillation de mod√®le
```

### 2. Am√©liorer la vitesse d'inf√©rence

Dans `ModelManager.kt`, ajustez:
```kotlin
// R√©duire max_seq_length
private val maxSeqLength = 256  // Au lieu de 512

// R√©duire maxNewTokens dans generate()
maxNewTokens = 50  // Au lieu de 100
```

### 3. Optimiser la m√©moire

Dans `app/build.gradle.kts`, ajoutez:
```kotlin
android {
    defaultConfig {
        // Limiter aux ABIs n√©cessaires
        ndk {
            abiFilters += listOf("arm64-v8a")  // Uniquement 64-bit
        }
    }
}
```

## ‚ùì Probl√®mes Courants

### "Le mod√®le ne charge pas"
- V√©rifiez que `model.ptl` est dans `app/src/main/assets/`
- V√©rifiez la taille: doit √™tre <200MB pour la plupart des appareils
- Regardez les logs Logcat pour l'erreur exacte

### "OutOfMemoryError"
- Utilisez un mod√®le quantifi√© (INT8)
- R√©duisez `maxSeqLength`
- Testez sur un appareil avec plus de RAM (6GB+)

### "G√©n√©ration trop lente"
- Quantifiez le mod√®le
- R√©duisez le nombre de tokens g√©n√©r√©s
- Testez sur un appareil plus r√©cent (Snapdragon 7xx+)

### "Tokens incompr√©hensibles"
- V√©rifiez que `tokenizer.json` correspond √† votre mod√®le
- Le tokenizer de d√©mo est tr√®s basique, utilisez votre vrai tokenizer

## üìö Ressources

- [Documentation PyTorch Mobile](https://pytorch.org/mobile/)
- [Guide Android Studio](https://developer.android.com/studio)
- [Optimisation des mod√®les](https://pytorch.org/tutorials/recipes/mobile_interpreter.html)

## üéì Prochaines √âtapes

1. **Am√©liorer le tokenizer**: Utilisez votre vrai tokenizer BPE
2. **Ajouter des prompts syst√®me**: Guides de conversation
3. **Sauvegarder l'historique**: Avec Room Database
4. **Partager les conversations**: Export en texte/JSON
5. **Mode sombre**: Th√®me personnalisable
6. **Voix**: Synth√®se vocale pour les r√©ponses

## üÜò Support

Si vous rencontrez des probl√®mes:
1. V√©rifiez les logs dans Logcat (Android Studio)
2. Testez le mod√®le avec `test_model.py` avant d√©ploiement
3. Commencez avec un petit mod√®le de test
4. Augmentez progressivement la taille

Bonne chance ! üöÄ
