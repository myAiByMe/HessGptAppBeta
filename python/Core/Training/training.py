import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import math
import os
from tqdm import tqdm
import pickle

# ============================================
# DATASET
# ============================================

class TextDataset(Dataset):
    """
    Dataset pour l'entra√Ænement de GPT-2/HessGPT
    Prend un long texte et le d√©coupe en s√©quences
    """
    def __init__(self, text, tokenizer, seq_len=128):
        """
        Args:
            text (str): Texte brut
            tokenizer: Votre tokenizer BPE
            seq_len (int): Longueur des s√©quences (128-256 pour commencer)
        """
        self.seq_len = seq_len
        self.tokenizer = tokenizer
        
        # Encoder tout le texte
        print("Encodage du texte...")
        self.tokens = tokenizer.encoder(text)
        print(f"‚úì {len(self.tokens)} tokens encod√©s")
        
        # Calculer le nombre de s√©quences possibles
        self.num_sequences = len(self.tokens) // seq_len
        
        # Tronquer pour avoir un multiple de seq_len
        self.tokens = self.tokens[:self.num_sequences * seq_len]
        
    def __len__(self):
        return self.num_sequences - 1  # -1 car on a besoin de input + target
    
    def __getitem__(self, idx):
        """
        Retourne une s√©quence et son target (d√©cal√© de 1)
        """
        start_idx = idx * self.seq_len
        end_idx = start_idx + self.seq_len
        
        # Input: tokens[start:end]
        input_ids = torch.tensor(self.tokens[start_idx:end_idx], dtype=torch.long)
        
        # Target: tokens[start+1:end+1] (d√©cal√© de 1 pour next token prediction)
        target_ids = torch.tensor(self.tokens[start_idx+1:end_idx+1], dtype=torch.long)
        
        return input_ids, target_ids


# ============================================
# TRAINER
# ============================================

class GPT2Trainer:
    """
    Classe pour entra√Æner GPT-2/HessGPT avec RoPE
    G√®re l'optimisation, la loss, les checkpoints, etc.
    """
    def __init__(
        self,
        model,
        train_dataset,
        val_dataset=None,
        learning_rate=3e-4,
        batch_size=4,
        num_epochs=10,
        device='cuda' if torch.cuda.is_available() else 'cpu',
        checkpoint_dir='./checkpoints',
        gradient_accumulation_steps=1,
        warmup_steps=100
    ):
        """
        Args:
            model: Votre mod√®le GPT-2/HessGPT (avec RoPE!)
            train_dataset: Dataset d'entra√Ænement
            val_dataset: Dataset de validation (optionnel)
            learning_rate: Taux d'apprentissage (3e-4 est bon pour GPT)
            batch_size: Taille des batches (4-8 sur Colab gratuit)
            num_epochs: Nombre d'epochs
            device: 'cuda' ou 'cpu'
            checkpoint_dir: Dossier pour sauvegarder les checkpoints
            gradient_accumulation_steps: Accumuler les gradients pour simuler un plus gros batch
            warmup_steps: Nombre de steps pour le warmup du learning rate
        """
        self.model = model.to(device)
        self.device = device
        self.num_epochs = num_epochs
        self.checkpoint_dir = checkpoint_dir
        self.gradient_accumulation_steps = gradient_accumulation_steps
        self.warmup_steps = warmup_steps
        
        # Cr√©er le dossier de checkpoints
        os.makedirs(checkpoint_dir, exist_ok=True)
        
        # DataLoaders
        self.train_loader = DataLoader(
            train_dataset,
            batch_size=batch_size,
            shuffle=True,
            num_workers=0  # 0 pour √©viter les probl√®mes sur Colab
        )
        
        self.val_loader = None
        if val_dataset is not None:
            self.val_loader = DataLoader(
                val_dataset,
                batch_size=batch_size,
                shuffle=False,
                num_workers=0
            )
        
        # Optimizer (AdamW comme dans GPT-2 original)
        self.optimizer = torch.optim.AdamW(
            model.parameters(),
            lr=learning_rate,
            betas=(0.9, 0.95),  # Valeurs utilis√©es dans GPT-2
            weight_decay=0.1
        )
        
        # Learning rate scheduler avec warmup
        total_steps = len(self.train_loader) * num_epochs // gradient_accumulation_steps
        
        # Warmup lin√©aire puis cosine decay
        self.scheduler = self._get_scheduler(learning_rate, total_steps, warmup_steps)
        
        # Historique
        self.train_losses = []
        self.val_losses = []
        self.global_step = 0
        
    def _get_scheduler(self, lr, total_steps, warmup_steps):
        """Cr√©e un scheduler avec warmup lin√©aire puis cosine decay"""
        def lr_lambda(step):
            if step < warmup_steps:
                # Warmup lin√©aire
                return step / warmup_steps
            else:
                # Cosine decay apr√®s warmup
                progress = (step - warmup_steps) / (total_steps - warmup_steps)
                return 0.5 * (1 + math.cos(math.pi * progress))
        
        return torch.optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda)
    
    def train_epoch(self, epoch):
        """Entra√Æne le mod√®le pour une epoch"""
        self.model.train()
        total_loss = 0
        
        pbar = tqdm(self.train_loader, desc=f"Epoch {epoch+1}/{self.num_epochs}")
        
        for batch_idx, (input_ids, target_ids) in enumerate(pbar):
            # D√©placer sur le device
            input_ids = input_ids.to(self.device)
            target_ids = target_ids.to(self.device)
            
            # Forward pass
            logits, loss = self.model(input_ids, target_ids)
            
            # Normaliser la loss par gradient_accumulation_steps
            loss = loss / self.gradient_accumulation_steps
            
            # Backward pass
            loss.backward()
            
            # Update weights tous les gradient_accumulation_steps
            if (batch_idx + 1) % self.gradient_accumulation_steps == 0:
                # Gradient clipping (important pour la stabilit√©)
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                
                # Update weights
                self.optimizer.step()
                self.scheduler.step()
                self.optimizer.zero_grad()
                self.global_step += 1
            
            # Tracking (multiplier par gradient_accumulation_steps pour avoir la vraie loss)
            actual_loss = loss.item() * self.gradient_accumulation_steps
            total_loss += actual_loss
            avg_loss = total_loss / (batch_idx + 1)
            
            # Update progress bar
            pbar.set_postfix({
                'loss': f'{actual_loss:.4f}',
                'avg_loss': f'{avg_loss:.4f}',
                'lr': f'{self.scheduler.get_last_lr()[0]:.6f}',
                'step': self.global_step
            })
        
        avg_epoch_loss = total_loss / len(self.train_loader)
        self.train_losses.append(avg_epoch_loss)
        
        return avg_epoch_loss
    
    def validate(self):
        """Valide le mod√®le"""
        if self.val_loader is None:
            return None
        
        self.model.eval()
        total_loss = 0
        
        with torch.no_grad():
            for input_ids, target_ids in tqdm(self.val_loader, desc="Validation"):
                input_ids = input_ids.to(self.device)
                target_ids = target_ids.to(self.device)
                
                logits, loss = self.model(input_ids, target_ids)
                total_loss += loss.item()
        
        avg_val_loss = total_loss / len(self.val_loader)
        self.val_losses.append(avg_val_loss)
        
        return avg_val_loss
    
    def save_checkpoint(self, epoch, loss):
        """Sauvegarde un checkpoint"""
        checkpoint_path = os.path.join(
            self.checkpoint_dir,
            f'checkpoint_epoch_{epoch+1}_loss_{loss:.4f}.pt'
        )
        
        torch.save({
            'epoch': epoch,
            'global_step': self.global_step,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'loss': loss,
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
        }, checkpoint_path)
        
        print(f"‚úì Checkpoint sauvegard√©: {checkpoint_path}")
    
    def load_checkpoint(self, checkpoint_path):
        """Charge un checkpoint"""
        checkpoint = torch.load(checkpoint_path, map_location=self.device)
        
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        self.train_losses = checkpoint['train_losses']
        self.val_losses = checkpoint['val_losses']
        self.global_step = checkpoint.get('global_step', 0)
        
        print(f"‚úì Checkpoint charg√©: {checkpoint_path}")
        print(f"  Epoch: {checkpoint['epoch']+1}")
        print(f"  Loss: {checkpoint['loss']:.4f}")
        print(f"  Global step: {self.global_step}")
        
        return checkpoint['epoch']
    
    def train(self, save_every=1):
        """
        Boucle d'entra√Ænement compl√®te
        
        Args:
            save_every (int): Sauvegarder un checkpoint tous les N epochs
        """
        print("\n" + "="*60)
        print("D√âBUT DE L'ENTRA√éNEMENT")
        print("="*60)
        print(f"Device: {self.device}")
        print(f"Nombre d'epochs: {self.num_epochs}")
        print(f"Batch size: {self.train_loader.batch_size}")
        print(f"Gradient accumulation steps: {self.gradient_accumulation_steps}")
        print(f"Effective batch size: {self.train_loader.batch_size * self.gradient_accumulation_steps}")
        print(f"Batches par epoch: {len(self.train_loader)}")
        
        # Afficher les d√©tails du mod√®le
        model_params = self.model.count_parameters() if hasattr(self.model, 'count_parameters') else None
        if model_params:
            print(f"\nüìä Param√®tres du mod√®le:")
            print(f"  - Total: {model_params['total']:,}")
            print(f"  - Token embeddings: {model_params['token_embeddings']:,}")
            print(f"  - Position embeddings: {model_params['position_embeddings']:,}")
            if model_params['position_embeddings'] == 0:
                print(f"    ‚ú® RoPE activ√© (0 param√®tres pour positions!)")
            print(f"  - Transformer blocks: {model_params['transformer_blocks']:,}")
        else:
            print(f"Param√®tres du mod√®le: {sum(p.numel() for p in self.model.parameters()):,}")
        
        print("="*60 + "\n")
        
        best_val_loss = float('inf')
        
        for epoch in range(self.num_epochs):
            # Entra√Ænement
            train_loss = self.train_epoch(epoch)
            
            # Validation
            val_loss = self.validate()
            
            # Affichage
            print(f"\nEpoch {epoch+1}/{self.num_epochs}")
            print(f"  Train Loss: {train_loss:.4f}")
            if val_loss is not None:
                print(f"  Val Loss:   {val_loss:.4f}")
            
            # Sauvegarder le checkpoint
            if (epoch + 1) % save_every == 0:
                self.save_checkpoint(epoch, train_loss)
            
            # Sauvegarder le meilleur mod√®le
            if val_loss is not None and val_loss < best_val_loss:
                best_val_loss = val_loss
                best_model_path = os.path.join(self.checkpoint_dir, 'best_model.pt')
                torch.save(self.model.state_dict(), best_model_path)
                print(f"  ‚úì Meilleur mod√®le sauvegard√©! (val_loss: {val_loss:.4f})")
        
        print("\n" + "="*60)
        print("ENTRA√éNEMENT TERMIN√â!")
        print("="*60)
        print(f"Train Loss finale: {self.train_losses[-1]:.4f}")
        if self.val_losses:
            print(f"Val Loss finale: {self.val_losses[-1]:.4f}")
            print(f"Meilleure Val Loss: {best_val_loss:.4f}")
        print("="*60 + "\n")


# ============================================
# EXEMPLE D'UTILISATION
# ============================================

def example_training():
    """Exemple complet d'entra√Ænement avec RoPE"""
    print("\nüöÄ EXEMPLE D'ENTRA√éNEMENT HessGPT AVEC RoPE\n")
    
    # 1. Importer le mod√®le et le tokenizer (vous devez adapter les imports)
    from Model.HessGpt import HessGPT
    from Tokenizer.Tokenizer import MYBPE
    
    # 2. Charger le tokenizer
    print("Chargement du tokenizer...")
    tokenizer = MYBPE(vocab_size=300)
    tokenizer.load_tokenizer("./Tokenizer/tokenizer_model.bin")
    print("‚úì Tokenizer charg√©")
    
    # 3. Charger les donn√©es
    print("\nChargement des donn√©es...")
    with open("./data/train.txt", "r", encoding="utf-8") as f:
        train_text = f.read()
    
    print(f"‚úì {len(train_text)} caract√®res charg√©s")
    
    # 4. Cr√©er les datasets
    print("\nCr√©ation des datasets...")
    train_dataset = TextDataset(train_text, tokenizer, seq_len=128)
    print(f"‚úì {len(train_dataset)} s√©quences d'entra√Ænement")
    
    # 5. Cr√©er le mod√®le AVEC RoPE
    print("\nCr√©ation du mod√®le avec RoPE...")
    model = HessGPT(
        vocab_size=300,
        embed_dim=256,      # Plus petit pour tester (768 pour le vrai)
        num_heads=8,        # Plus petit pour tester (12 pour le vrai)
        num_layers=4,       # Plus petit pour tester (12 pour le vrai)
        max_seq_len=128,
        use_rope=True       # ‚ú® RoPE activ√©!
    )
    
    params = model.count_parameters()
    print(f"‚úì Mod√®le cr√©√© ({params['total']:,} param√®tres)")
    print(f"  - Position embeddings: {params['position_embeddings']:,} (RoPE = 0!)")
    
    # 6. Cr√©er le trainer
    trainer = GPT2Trainer(
        model=model,
        train_dataset=train_dataset,
        learning_rate=3e-4,
        batch_size=4,
        num_epochs=5,
        device='cuda' if torch.cuda.is_available() else 'cpu',
        gradient_accumulation_steps=2,  # Batch effectif = 4 * 2 = 8
        warmup_steps=100
    )
    
    # 7. Entra√Æner!
    trainer.train(save_every=1)
    
    print("\n‚úì Entra√Ænement termin√©!")
    print("‚úì Les checkpoints sont dans ./checkpoints/")


def example_training_20m():
    """Exemple d'entra√Ænement avec configuration 20M param√®tres"""
    print("\nüöÄ EXEMPLE D'ENTRA√éNEMENT HessGPT 20M AVEC RoPE\n")
    
    from Model.HessGpt import HessGPT
    from Tokenizer.Tokenizer import MYBPE
    
    # 1. Charger le tokenizer (20k vocab)
    print("Chargement du tokenizer...")
    tokenizer = MYBPE(vocab_size=20000)
    tokenizer.load_tokenizer("./Tokenizer/tokenizer_model.bin")
    print("‚úì Tokenizer charg√©")
    
    # 2. Charger les donn√©es
    print("\nChargement des donn√©es...")
    with open("./data/train.txt", "r", encoding="utf-8") as f:
        train_text = f.read()
    
    print(f"‚úì {len(train_text)} caract√®res charg√©s")
    
    # 3. Cr√©er les datasets
    print("\nCr√©ation des datasets...")
    train_dataset = TextDataset(train_text, tokenizer, seq_len=512)
    print(f"‚úì {len(train_dataset)} s√©quences d'entra√Ænement")
    
    # 4. Cr√©er le mod√®le 20M avec RoPE
    print("\nCr√©ation du mod√®le 20M avec RoPE...")
    model = HessGPT(
        vocab_size=20000,
        embed_dim=512,
        num_heads=8,
        num_layers=6,
        max_seq_len=2048,
        use_rope=True  # ‚ú® RoPE!
    )
    
    params = model.count_parameters()
    print(f"\n‚úì Mod√®le cr√©√©:")
    print(f"  - Total: {params['total']:,} param√®tres")
    print(f"  - Token embeddings: {params['token_embeddings']:,}")
    print(f"  - Position embeddings: {params['position_embeddings']:,} ‚ú®")
    print(f"  - Transformer blocks: {params['transformer_blocks']:,}")
    
    # 5. Cr√©er le trainer
    trainer = GPT2Trainer(
        model=model,
        train_dataset=train_dataset,
        learning_rate=3e-4,
        batch_size=2,          # Plus petit car mod√®le plus gros
        num_epochs=10,
        device='cuda' if torch.cuda.is_available() else 'cpu',
        gradient_accumulation_steps=4,  # Batch effectif = 2 * 4 = 8
        warmup_steps=500
    )
    
    # 6. Entra√Æner!
    trainer.train(save_every=2)
    
    print("\n‚úì Entra√Ænement 20M termin√©!")


def test_dataset():
    """Test simple du dataset"""
    print("\n" + "="*60)
    print("TEST: Dataset")
    print("="*60)
    
    # Cr√©er un mini texte de test
    test_text = "Bonjour! " * 100  # R√©p√©ter pour avoir assez de tokens
    
    # Mock tokenizer pour le test
    class MockTokenizer:
        def encoder(self, text):
            # Simuler l'encodage (1 token par caract√®re pour simplifier)
            return list(range(len(test_text)))
    
    tokenizer = MockTokenizer()
    
    # Cr√©er le dataset
    dataset = TextDataset(test_text, tokenizer, seq_len=10)
    
    print(f"‚úì Dataset cr√©√©: {len(dataset)} s√©quences")
    
    # Tester __getitem__
    input_ids, target_ids = dataset[0]
    
    print(f"‚úì Input shape: {input_ids.shape}")
    print(f"‚úì Target shape: {target_ids.shape}")
    print(f"‚úì Input:  {input_ids.tolist()[:10]}")
    print(f"‚úì Target: {target_ids.tolist()[:10]}")
    print(f"  (Target = Input d√©cal√© de 1)")


if __name__ == "__main__":
    print("\nüöÄ TRAINING LOOP - HessGPT AVEC RoPE\n")
    
    # Test du dataset
    test_dataset()
    
    print("\n" + "="*60)
    print("Pour lancer l'entra√Ænement complet:")
    print("="*60)
    print("1. Assurez-vous d'avoir un fichier data/train.txt")
    print("2. D√©commentez example_training() ou example_training_20m() ci-dessous")
    print("3. Ajustez les param√®tres (batch_size, epochs, etc.)")
    print("4. Lancez: python Training/training.py")
    print("\nüí° AVANTAGES DE RoPE:")
    print("  ‚Ä¢ Moins de param√®tres √† entra√Æner")
    print("  ‚Ä¢ Meilleure extrapolation aux longues s√©quences")
    print("  ‚Ä¢ Architecture moderne (LLaMA-style)")
    print("="*60)
    
    # D√©commentez pour lancer l'entra√Ænement:
    # example_training()          # Petit mod√®le de test
    # example_training_20m()      # Mod√®le 20M
