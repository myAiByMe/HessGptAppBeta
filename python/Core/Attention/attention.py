# attention.py
import torch
import torch.nn as nn
import torch.nn.functional as F
import math

class RotaryPositionalEmbedding(nn.Module):
    """
    RoPE (Rotary Position Embedding)
    Applique une rotation aux embeddings Q et K basÃ©e sur la position
    """
    def __init__(self, dim, max_seq_len=2048, base=10000):
        """
        Args:
            dim (int): Dimension par tÃªte (head_dim)
            max_seq_len (int): Longueur maximale de sÃ©quence
            base (int): Base pour le calcul des frÃ©quences
        """
        super().__init__()
        self.dim = dim
        self.max_seq_len = max_seq_len
        self.base = base
        
        # PrÃ©calculer les frÃ©quences
        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))
        self.register_buffer('inv_freq', inv_freq)
        
        # Cache pour les embeddings prÃ©calculÃ©s
        self._seq_len_cached = None
        self._cos_cached = None
        self._sin_cached = None
    
    def _update_cos_sin_cache(self, seq_len, device, dtype):
        """Met Ã  jour le cache cos/sin si nÃ©cessaire"""
        if seq_len != self._seq_len_cached:
            self._seq_len_cached = seq_len
            
            # CrÃ©er les positions
            t = torch.arange(seq_len, device=device, dtype=dtype)
            
            # Calculer les frÃ©quences pour chaque position
            freqs = torch.outer(t, self.inv_freq.to(dtype))
            
            # CrÃ©er les embeddings avec rÃ©pÃ©tition pour chaque paire de dimensions
            emb = torch.cat((freqs, freqs), dim=-1)
            
            self._cos_cached = emb.cos()
            self._sin_cached = emb.sin()
        
        return self._cos_cached, self._sin_cached
    
    def rotate_half(self, x):
        """
        Rotation de la moitiÃ© des dimensions
        [x1, x2, x3, x4, ...] -> [-x2, x1, -x4, x3, ...]
        """
        x1 = x[..., : x.shape[-1] // 2]
        x2 = x[..., x.shape[-1] // 2 :]
        return torch.cat((-x2, x1), dim=-1)
    
    def apply_rotary_pos_emb(self, q, k):
        """
        Applique RoPE Ã  Q et K
        
        Args:
            q: [batch_size, num_heads, seq_len, head_dim]
            k: [batch_size, num_heads, seq_len, head_dim]
        
        Returns:
            q_rot, k_rot avec positions encodÃ©es
        """
        seq_len = q.shape[2]
        
        # Obtenir cos et sin
        cos, sin = self._update_cos_sin_cache(seq_len, q.device, q.dtype)
        
        # Ajouter dimensions pour batch et heads: [1, 1, seq_len, head_dim]
        cos = cos[None, None, :, :]
        sin = sin[None, None, :, :]
        
        # Appliquer la rotation
        q_rot = (q * cos) + (self.rotate_half(q) * sin)
        k_rot = (k * cos) + (self.rotate_half(k) * sin)
        
        return q_rot, k_rot
    
    def forward(self, q, k):
        """Forward pass - applique RoPE"""
        return self.apply_rotary_pos_emb(q, k)


class SelfAttention(nn.Module):
    """
    Self-Attention simple (1 seule tÃªte) avec RoPE
    Pour comprendre les bases avant le Multi-Head
    """
    def __init__(self, embed_dim, use_rope=True, max_seq_len=2048):
        """
        Args:
            embed_dim (int): Dimension des embeddings (ex: 768)
            use_rope (bool): Utiliser RoPE ou pas
            max_seq_len (int): Longueur maximale de sÃ©quence
        """
        super().__init__()
        self.embed_dim = embed_dim
        self.use_rope = use_rope
        
        # Projections linÃ©aires pour Q, K, V
        self.query = nn.Linear(embed_dim, embed_dim)
        self.key = nn.Linear(embed_dim, embed_dim)
        self.value = nn.Linear(embed_dim, embed_dim)
        
        # RoPE
        if use_rope:
            self.rope = RotaryPositionalEmbedding(embed_dim, max_seq_len)
        
    def forward(self, x, mask=None):
        """
        Args:
            x: [batch_size, seq_len, embed_dim] - Les embeddings
            mask: [seq_len, seq_len] - Masque causal (optionnel)
        
        Returns:
            output: [batch_size, seq_len, embed_dim]
            attention_weights: [batch_size, seq_len, seq_len] - Pour visualisation
        """
        batch_size, seq_len, embed_dim = x.shape
        
        # 1. CrÃ©er Q, K, V
        Q = self.query(x)  # [batch_size, seq_len, embed_dim]
        K = self.key(x)    # [batch_size, seq_len, embed_dim]
        V = self.value(x)  # [batch_size, seq_len, embed_dim]
        
        # 2. Appliquer RoPE si activÃ©
        if self.use_rope:
            # Ajouter dimension pour num_heads (1 tÃªte ici)
            Q = Q.unsqueeze(1)  # [batch_size, 1, seq_len, embed_dim]
            K = K.unsqueeze(1)
            Q, K = self.rope(Q, K)
            Q = Q.squeeze(1)  # [batch_size, seq_len, embed_dim]
            K = K.squeeze(1)
        
        # 3. Calculer les scores d'attention
        # Q @ K^T = [batch_size, seq_len, seq_len]
        scores = torch.matmul(Q, K.transpose(-2, -1))
        
        # 4. Scaling (diviser par racine de dim)
        scores = scores / math.sqrt(embed_dim)
        
        # 5. Appliquer le masque causal (si fourni)
        if mask is not None:
            if mask.device != scores.device:
                mask = mask.to(scores.device)
            scores = scores.masked_fill(mask == 0, float('-inf'))
        
        # 6. Softmax pour obtenir les poids d'attention
        attention_weights = F.softmax(scores, dim=-1)
        
        # 7. Appliquer l'attention sur les Values
        output = torch.matmul(attention_weights, V)
        
        return output, attention_weights


class MultiHeadAttention(nn.Module):
    """
    Multi-Head Self-Attention avec RoPE
    Version amÃ©liorÃ©e avec Rotary Position Embeddings
    """
    def __init__(self, embed_dim, num_heads, dropout=0.1, use_rope=True, max_seq_len=2048):
        """
        Args:
            embed_dim (int): Dimension des embeddings (768 pour GPT-2 small)
            num_heads (int): Nombre de tÃªtes (12 pour GPT-2 small)
            dropout (float): Taux de dropout pour l'attention
            use_rope (bool): Utiliser RoPE (recommandÃ©!)
            max_seq_len (int): Longueur maximale de sÃ©quence
        """
        super().__init__()
        assert embed_dim % num_heads == 0, "embed_dim doit Ãªtre divisible par num_heads"
        
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads  # 768 // 12 = 64
        self.use_rope = use_rope
        
        # Projections Q, K, V (pour toutes les tÃªtes en une fois)
        self.qkv_proj = nn.Linear(embed_dim, 3 * embed_dim)
        
        # Projection de sortie
        self.out_proj = nn.Linear(embed_dim, embed_dim)
        
        # Dropout pour l'attention (comme GPT-2)
        self.attn_dropout = nn.Dropout(dropout)
        self.resid_dropout = nn.Dropout(dropout)
        
        # RoPE
        if use_rope:
            self.rope = RotaryPositionalEmbedding(self.head_dim, max_seq_len)
        
    def forward(self, x, mask=None):
        """
        Args:
            x: [batch_size, seq_len, embed_dim]
            mask: [seq_len, seq_len] - Masque causal
        
        Returns:
            output: [batch_size, seq_len, embed_dim]
        """
        batch_size, seq_len, embed_dim = x.shape
        
        # 1. Projeter en Q, K, V (toutes les tÃªtes d'un coup)
        qkv = self.qkv_proj(x)  # [batch_size, seq_len, 3 * embed_dim]
        
        # 2. SÃ©parer Q, K, V
        qkv = qkv.reshape(batch_size, seq_len, 3, self.num_heads, self.head_dim)
        qkv = qkv.permute(2, 0, 3, 1, 4)  # [3, batch_size, num_heads, seq_len, head_dim]
        Q, K, V = qkv[0], qkv[1], qkv[2]
        
        # 3. Appliquer RoPE Ã  Q et K
        if self.use_rope:
            Q, K = self.rope(Q, K)
        
        # 4. Calculer les scores d'attention
        # Q @ K^T : [batch_size, num_heads, seq_len, seq_len]
        scores = torch.matmul(Q, K.transpose(-2, -1))
        
        # 5. Scaling
        scores = scores / math.sqrt(self.head_dim)
        
        # 6. Appliquer le masque causal
        if mask is not None:
            if mask.device != scores.device:
                mask = mask.to(scores.device)
            
            # Broadcasting du masque
            mask = mask.unsqueeze(0).unsqueeze(0)  # [1, 1, seq_len, seq_len]
            scores = scores.masked_fill(mask == 0, float('-inf'))
        
        # 7. Softmax
        attention_weights = F.softmax(scores, dim=-1)
        
        # 8. Dropout sur les poids d'attention
        attention_weights = self.attn_dropout(attention_weights)
        
        # 9. Appliquer l'attention sur V
        output = torch.matmul(attention_weights, V)
        # output: [batch_size, num_heads, seq_len, head_dim]
        
        # 10. Recombiner les tÃªtes
        output = output.transpose(1, 2)  # [batch_size, seq_len, num_heads, head_dim]
        output = output.reshape(batch_size, seq_len, embed_dim)
        
        # 11. Projection finale avec dropout
        output = self.out_proj(output)
        output = self.resid_dropout(output)
        
        return output


def create_causal_mask(seq_len, device='cpu'):
    """
    CrÃ©e un masque causal (triangulaire infÃ©rieur)
    
    Args:
        seq_len (int): Longueur de la sÃ©quence
        device (str): Device sur lequel crÃ©er le masque
    
    Returns:
        mask: [seq_len, seq_len] - 1 pour visible, 0 pour masquÃ©
    
    Exemple pour seq_len=3:
        [[1, 0, 0],
         [1, 1, 0],
         [1, 1, 1]]
    """
    mask = torch.tril(torch.ones(seq_len, seq_len, device=device))
    return mask


# ============================================
# TESTS
# ============================================

def test_rope():
    """Test du module RoPE"""
    print("\n" + "="*60)
    print("TEST 0: Rotary Position Embedding (RoPE)")
    print("="*60)
    
    batch_size = 2
    num_heads = 4
    seq_len = 8
    head_dim = 64
    
    # CrÃ©er le module RoPE
    rope = RotaryPositionalEmbedding(head_dim, max_seq_len=1024)
    
    # CrÃ©er Q et K
    Q = torch.randn(batch_size, num_heads, seq_len, head_dim)
    K = torch.randn(batch_size, num_heads, seq_len, head_dim)
    
    print(f"âœ“ Q shape avant RoPE: {Q.shape}")
    print(f"âœ“ K shape avant RoPE: {K.shape}")
    
    # Appliquer RoPE
    Q_rot, K_rot = rope(Q, K)
    
    print(f"âœ“ Q shape aprÃ¨s RoPE: {Q_rot.shape}")
    print(f"âœ“ K shape aprÃ¨s RoPE: {K_rot.shape}")
    
    # VÃ©rifier que les dimensions sont prÃ©servÃ©es
    assert Q_rot.shape == Q.shape, "RoPE doit prÃ©server les dimensions"
    assert K_rot.shape == K.shape, "RoPE doit prÃ©server les dimensions"
    
    # VÃ©rifier que les valeurs ont changÃ©
    diff_q = (Q - Q_rot).abs().mean()
    diff_k = (K - K_rot).abs().mean()
    print(f"\nâœ“ DiffÃ©rence moyenne Q: {diff_q:.4f} (devrait Ãªtre > 0)")
    print(f"âœ“ DiffÃ©rence moyenne K: {diff_k:.4f} (devrait Ãªtre > 0)")
    
    print("\nâœ… RoPE fonctionne correctement!")


def test_self_attention():
    """Test de la Self-Attention simple avec RoPE"""
    print("\n" + "="*60)
    print("TEST 1: Self-Attention Simple avec RoPE")
    print("="*60)
    
    # ParamÃ¨tres
    batch_size = 2
    seq_len = 5
    embed_dim = 64
    
    # Test AVEC RoPE
    print("\nğŸ“ AVEC RoPE:")
    attention_rope = SelfAttention(embed_dim, use_rope=True)
    x = torch.randn(batch_size, seq_len, embed_dim)
    mask = create_causal_mask(seq_len, device=x.device)
    output_rope, attn_weights_rope = attention_rope(x, mask)
    
    print(f"  âœ“ Input shape: {x.shape}")
    print(f"  âœ“ Output shape: {output_rope.shape}")
    print(f"  âœ“ Attention weights shape: {attn_weights_rope.shape}")
    
    # Test SANS RoPE pour comparaison
    print("\nğŸ“ SANS RoPE (pour comparaison):")
    attention_no_rope = SelfAttention(embed_dim, use_rope=False)
    output_no_rope, attn_weights_no_rope = attention_no_rope(x, mask)
    
    print(f"  âœ“ Output shape: {output_no_rope.shape}")
    
    # Comparer les rÃ©sultats
    diff = (output_rope - output_no_rope).abs().mean()
    print(f"\nğŸ“Š DiffÃ©rence moyenne entre avec/sans RoPE: {diff:.4f}")
    print("   (Les outputs sont diffÃ©rents, c'est normal!)")


def test_multi_head_attention():
    """Test du Multi-Head Attention avec RoPE"""
    print("\n" + "="*60)
    print("TEST 2: Multi-Head Attention avec RoPE")
    print("="*60)
    
    # ParamÃ¨tres GPT-2 small
    batch_size = 2
    seq_len = 10
    embed_dim = 768
    num_heads = 12
    
    # CrÃ©er le module AVEC RoPE
    print("\nğŸ“ AVEC RoPE:")
    attention_rope = MultiHeadAttention(embed_dim, num_heads, use_rope=True)
    x = torch.randn(batch_size, seq_len, embed_dim)
    mask = create_causal_mask(seq_len, device=x.device)
    output_rope = attention_rope(x, mask)
    
    print(f"  âœ“ Input shape: {x.shape}")
    print(f"  âœ“ Output shape: {output_rope.shape}")
    print(f"  âœ“ Nombre de tÃªtes: {num_heads}")
    print(f"  âœ“ Dimension par tÃªte: {embed_dim // num_heads}")
    
    # Nombre de paramÃ¨tres
    num_params = sum(p.numel() for p in attention_rope.parameters())
    print(f"\n  âœ“ Nombre de paramÃ¨tres: {num_params:,}")
    
    # Test SANS RoPE pour comparaison
    print("\nğŸ“ SANS RoPE (pour comparaison):")
    attention_no_rope = MultiHeadAttention(embed_dim, num_heads, use_rope=False)
    output_no_rope = attention_no_rope(x, mask)
    
    num_params_no_rope = sum(p.numel() for p in attention_no_rope.parameters())
    print(f"  âœ“ Nombre de paramÃ¨tres: {num_params_no_rope:,}")
    print(f"  âœ“ DiffÃ©rence de paramÃ¨tres: {num_params - num_params_no_rope:,}")
    print("    (RoPE n'ajoute PAS de paramÃ¨tres entraÃ®nables!)")


def test_rope_extrapolation():
    """Test de l'extrapolation de RoPE Ã  des sÃ©quences plus longues"""
    print("\n" + "="*60)
    print("TEST 3: Extrapolation RoPE (sÃ©quences longues)")
    print("="*60)
    
    batch_size = 1
    num_heads = 8
    embed_dim = 512
    head_dim = embed_dim // num_heads
    
    # EntraÃ®nÃ© sur seq_len=512
    max_seq_len_train = 512
    
    # Tester sur seq_len=1024 (extrapolation!)
    seq_len_test = 1024
    
    print(f"âœ“ Longueur max d'entraÃ®nement: {max_seq_len_train}")
    print(f"âœ“ Longueur de test: {seq_len_test}")
    
    # CrÃ©er RoPE
    rope = RotaryPositionalEmbedding(head_dim, max_seq_len=max_seq_len_train)
    
    # CrÃ©er attention avec RoPE
    attention = MultiHeadAttention(embed_dim, num_heads, use_rope=True, max_seq_len=max_seq_len_train)
    
    # Tester avec une sÃ©quence plus longue
    x = torch.randn(batch_size, seq_len_test, embed_dim)
    mask = create_causal_mask(seq_len_test)
    
    try:
        output = attention(x, mask)
        print(f"\nâœ… RoPE peut extrapoler Ã  {seq_len_test} tokens!")
        print(f"   Output shape: {output.shape}")
    except Exception as e:
        print(f"\nâŒ Erreur d'extrapolation: {e}")


def test_with_cuda():
    """Test avec CUDA si disponible"""
    print("\n" + "="*60)
    print("TEST 4: Test avec CUDA (si disponible)")
    print("="*60)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"âœ“ Device: {device}")
    
    if device.type == 'cpu':
        print("  âš ï¸  CUDA non disponible, test sur CPU")
    
    # ParamÃ¨tres
    batch_size = 2
    seq_len = 10
    embed_dim = 256
    num_heads = 8
    
    # CrÃ©er le module et le mettre sur le device
    attention = MultiHeadAttention(embed_dim, num_heads, use_rope=True).to(device)
    
    # Input sur le device
    x = torch.randn(batch_size, seq_len, embed_dim).to(device)
    
    # Masque sur le device
    mask = create_causal_mask(seq_len, device=device)
    
    # Forward
    output = attention(x, mask)
    
    print(f"âœ“ Input device: {x.device}")
    print(f"âœ“ Mask device: {mask.device}")
    print(f"âœ“ Output device: {output.device}")
    print(f"âœ“ Output shape: {output.shape}")


if __name__ == "__main__":
    print("\nğŸš€ TESTS DE LA SELF-ATTENTION AVEC RoPE\n")
    
    # Test 0: RoPE seul
    test_rope()
    
    # Test 1: Attention simple
    test_self_attention()
    
    # Test 2: Multi-Head Attention
    test_multi_head_attention()
    
    # Test 3: Extrapolation
    test_rope_extrapolation()
    
    # Test 4: Avec CUDA
    test_with_cuda()
    
    print("\n" + "="*60)
    print("âœ… TOUS LES TESTS PASSÃ‰S!")
    print("="*60)
    print("\nğŸ”§ MODIFICATIONS APPLIQUÃ‰ES:")
    print("  1. âœ¨ RoPE (Rotary Position Embedding) ajoutÃ©")
    print("  2. ğŸ¯ Pas de paramÃ¨tres entraÃ®nables supplÃ©mentaires")
    print("  3. ğŸš€ Meilleure extrapolation aux sÃ©quences longues")
    print("  4. âš¡ Performance identique, qualitÃ© amÃ©liorÃ©e")
    print("\nğŸ’¡ AVANTAGES DE RoPE:")
    print("  â€¢ Encode la position directement dans Q et K")
    print("  â€¢ Pas besoin d'embeddings positionnels sÃ©parÃ©s")
    print("  â€¢ Meilleure gÃ©nÃ©ralisation aux longueurs non vues")
    print("  â€¢ UtilisÃ© par LLaMA, PaLM, GPT-NeoX, etc.")
    print("\nğŸ“ PrÃªt pour transformersblock.py!")
    print("="*60 + "\n")
