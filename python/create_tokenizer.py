#!/usr/bin/env python3
"""
CrÃ©ation du tokenizer.json pour HessGPT Mobile
GPT-2 tokenizer + 4 tokens ChatLM
"""

import json
import argparse
from transformers import GPT2Tokenizer

# Tokens spÃ©ciaux ChatLM (EXACTEMENT comme dans pretrain)
SPECIAL_TOKENS = {
    '<|system|>':    50257,
    '<|user|>':      50258,
    '<|assistant|>': 50259,
    '<|end|>':       50260,
}

def create_hessgpt_tokenizer(output_path):
    """
    CrÃ©e le tokenizer.json pour HessGPT Mobile
    """
    
    print("=" * 80)
    print("ğŸ“ CRÃ‰ATION TOKENIZER HESSGPT")
    print("=" * 80)
    
    # 1. Charger GPT-2 tokenizer
    print("\nğŸ“¥ TÃ©lÃ©chargement tokenizer GPT-2...")
    tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
    
    # 2. Ajouter les tokens spÃ©ciaux
    print(f"\nâœ¨ Ajout des tokens ChatLM...")
    tokenizer.add_special_tokens({
        'additional_special_tokens': list(SPECIAL_TOKENS.keys())
    })
    
    # 3. Construire le vocabulaire complet
    print(f"\nğŸ—ï¸  Construction du vocabulaire...")
    vocab = tokenizer.get_vocab()
    
    print(f"   â€¢ Vocab GPT-2        : {50257:,} tokens")
    print(f"   â€¢ Tokens ChatLM      : {len(SPECIAL_TOKENS)} tokens")
    print(f"   â€¢ Total              : {len(vocab):,} tokens")
    
    # 4. VÃ©rifier que les tokens spÃ©ciaux ont les bons IDs
    print(f"\nğŸ” VÃ©rification des IDs :")
    for token, expected_id in SPECIAL_TOKENS.items():
        actual_id = vocab.get(token)
        status = "âœ…" if actual_id == expected_id else "âŒ"
        print(f"   {status} {token:20s} â†’ {actual_id} (attendu: {expected_id})")
    
    # 5. RÃ©cupÃ©rer les merges BPE
    print(f"\nğŸ“‹ Extraction des merges BPE...")
    merges = []
    if hasattr(tokenizer, 'bpe_ranks'):
        merges = [f"{a} {b}" for (a, b) in tokenizer.bpe_ranks.keys()]
    elif hasattr(tokenizer, 'encoder') and hasattr(tokenizer, 'bpe_ranks'):
        # Alternative pour certaines versions de transformers
        bpe_file = tokenizer.vocab_files_names.get('merges_file', 'merges.txt')
        # Les merges sont stockÃ©s dans le modÃ¨le
        merges = []
    
    print(f"   âœ… {len(merges):,} merges BPE")
    
    # 6. Construire la structure JSON
    tokenizer_data = {
        "vocab": vocab,
        "merges": merges,
        "model_max_length": 512,  # Comme dans pretrain
        "added_tokens": [
            {"content": token, "id": token_id}
            for token, token_id in SPECIAL_TOKENS.items()
        ],
        "special_tokens": {
            "bos_token": tokenizer.bos_token,
            "eos_token": tokenizer.eos_token,
            "unk_token": tokenizer.unk_token,
            "pad_token": tokenizer.pad_token if tokenizer.pad_token else tokenizer.eos_token,
        },
        "model_type": "HessGPT",
        "version": "1.0",
    }
    
    # 7. Sauvegarder
    print(f"\nğŸ’¾ Sauvegarde : {output_path}")
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(tokenizer_data, f, ensure_ascii=False, indent=2)
    
    # 8. Test
    print(f"\nğŸ§ª Test du tokenizer...")
    test_texts = [
        "Bonjour, comment Ã§a va ?",
        "<|user|>Hello<|end|>",
        "<|assistant|>Salut !<|end|>",
    ]
    
    for text in test_texts:
        encoded = tokenizer.encode(text)
        decoded = tokenizer.decode(encoded)
        print(f"\n   ğŸ“ '{text}'")
        print(f"   â†’ IDs : {encoded[:10]}{'...' if len(encoded) > 10 else ''}")
        print(f"   â†’ DÃ©codÃ© : '{decoded}'")
    
    print("\n" + "=" * 80)
    print("âœ… TOKENIZER CRÃ‰Ã‰ !")
    print("=" * 80)
    print(f"\nğŸ“¦ Fichier  : {output_path}")
    print(f"ğŸ“Š Tokens   : {len(vocab):,}")
    print(f"ğŸ¯ PrÃªt pour Android !")
    
    print(f"\nğŸ“± Prochaine Ã©tape :")
    print(f"   cp {output_path} HessGptMobileApp/app/src/main/assets/")


def main():
    parser = argparse.ArgumentParser(
        description="CrÃ©er tokenizer.json pour HessGPT Mobile"
    )
    parser.add_argument(
        '--output',
        type=str,
        default='tokenizer.json',
        help='Fichier de sortie'
    )
    
    args = parser.parse_args()
    
    create_hessgpt_tokenizer(args.output)


if __name__ == "__main__":
    main()