#!/usr/bin/env python3
"""
ğŸš€ HessGPT - PRE-TRAIN SCALABLE (Chunk-Based) avec RoPE
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âœ… Load directement les chunks du downloader (mixed_1B_chunk_X/)
âœ… Fusion des .pt (fineweb_edu.pt + wikipedia.pt + ...) en sÃ©quence
âœ… BF16 training (plus rapide + plus stable que FP16 sur modern GPU)
âœ… RoPE (Rotary Position Embeddings) â€” Ã©conomise ~10M paramÃ¨tres
âœ… LR WSD dynamique â€” scale Ã  l'infini, decay uniquement Ã  la fin
   â†’ InspirÃ© de Qwen : warmup 2% / stable 90%+ / decay 8%
   â†’ Tu peux rajouter des chunks sans toucher au schedule
âœ… 1 epoch = 1 chunk chargÃ© en RAM puis libÃ©rÃ©
âœ… Reprise propre via checkpoint (chunk_id sauvegardÃ©)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

USAGE:
    python pretrain_hessgpt.py                    # Start / resume normal
    python pretrain_hessgpt.py --total-chunks 50  # Override nombre de chunks
    python pretrain_hessgpt.py --dry-run          # VÃ©rifie les chunks sans trainer
"""

import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset
import sys
import os
import time
import math
import json
import gc
import argparse
from tqdm import tqdm
from transformers import GPT2Tokenizer
from datetime import datetime
import traceback

sys.path.append('./Core/Model')

from HessGpt import HessGPT
# ============================================
# TOKENS SPÃ‰CIAUX CHATLM
# ============================================
# Ces 4 tokens sont rÃ©servÃ©s pour la phase fine-tuning ChatLM.
# On les bake dans le vocab DÃˆS le pre-training pour que les
# embeddings existent dÃ©jÃ  et soient initialisÃ©s proprement.
# Le pre-training ne les utilise pas dans les sÃ©quences,
# mais le modÃ¨le apprend leurs embeddings via les autres tokens.
SPECIAL_TOKENS = {
    '<|system|>':    50257,
    '<|user|>':      50258,
    '<|assistant|>': 50259,
    '<|end|>':       50260,
}

# ============================================
# ARGS CLI
# ============================================
parser = argparse.ArgumentParser(description='HessGPT Scalable Pre-Training')
parser.add_argument('--total-chunks', type=int, default=None,
                    help='Override nombre de chunks Ã  train (auto-dÃ©tectÃ© sinon)')
parser.add_argument('--dry-run', action='store_true',
                    help='VÃ©rifie les chunks sans lancer le training')
parser.add_argument('--data-dir', type=str, default='./data/mixed',
                    help='Directory des chunks du downloader')
parser.add_argument('--checkpoint', type=str, default='./checkpoints/HessGpt_pretrain.pt',
                    help='Path du checkpoint')
args = parser.parse_args()

print("=" * 80)
print("ğŸš€ HessGPT â€” SCALABLE PRE-TRAINING avec RoPE")
print("   BF16 | WSD Dynamic LR | Chunk-Based Loading | Rotary Position Embeddings")
print("=" * 80)

# ============================================
# CONFIGURATION
# ============================================
CONFIG = {
    # --- Model (0.5B params avec RoPE) ---
    'vocab_size':    50257 + len(SPECIAL_TOKENS),  # 50261 (GPT2 + 4 ChatLM tokens)
    'embed_dim':     1280,
    'num_heads':     20,
    'num_layers':    20,
    'max_seq_len':   512,
    'dropout':       0.1,
    'use_rope':      True,  # âœ¨ RoPE (Rotary Position Embeddings) - Ã©conomise ~10M params

    # --- Training ---
    'batch_size':              32,   # H100 80GB + modÃ¨le 0.5B (~6GB) â†’ on peut faire 32 facilement
    'gradient_accumulation':   4,    # effective batch = 32Ã—4Ã—512 = 65536 tokens
    'max_grad_norm':           1.0,
    'learning_rate':           3e-4,

    # --- Data ---
    'data_dir':    args.data_dir,       # ./data/mixed/
    'val_ratio':   0.005,               # 0.5% du chunk 0 pour validation

    # --- WSD LR Schedule (DYNAMIQUE) ---
    # Le schedule se calcule en fonction du TOTAL de chunks dÃ©tectÃ©.
    # Tu peux rajouter des chunks : le decay se pousse automatiquement Ã  la fin.
    # InspirÃ© Qwen : warmup trÃ¨s court, stable trÃ¨s long, decay Ã  la fin.
    'warmup_ratio':  0.02,    # 2% des steps totaux (â‰ˆ 1 chunk sur 50)
    'decay_ratio':   0.08,    # 8% des steps totaux (â‰ˆ 4 chunks sur 50)
    # stable_ratio = 1.0 - warmup - decay (calculÃ© auto)
    'min_lr_ratio':  0.1,     # LR_min = LR_max * 0.1 Ã  la fin du decay

    # --- Validation ---
    'validate_every_steps': 500,
    'val_batches':          50,

    # --- Checkpoint ---
    'checkpoint_file':     args.checkpoint,
    'save_every_epochs':   5,

    # --- System ---
    'use_compile':    True,
    'compile_mode':   'default',
}

device = 'cuda' if torch.cuda.is_available() else 'cpu'

print(f"\nğŸ“Š CONFIGURATION :")
print(f"   Vocab size : {CONFIG['vocab_size']:,} (GPT2 50257 + {len(SPECIAL_TOKENS)} ChatLM)")
print(f"   Embed dim  : {CONFIG['embed_dim']}")
print(f"   Layers     : {CONFIG['num_layers']}")
print(f"   Heads      : {CONFIG['num_heads']}")
print(f"   Seq len    : {CONFIG['max_seq_len']}")
print(f"   Use RoPE   : {CONFIG['use_rope']} âœ¨" if CONFIG['use_rope'] else f"   Use RoPE   : {CONFIG['use_rope']}")

print(f"\nğŸ—£ï¸  TOKENS CHATLM :")
for token, idx in SPECIAL_TOKENS.items():
    print(f"   {token:20s} â†’ id {idx}")

# ============================================
# SCAN CHUNKS DISPONIBLES
# ============================================
def scan_available_chunks(data_dir):
    """
    Scanne le data_dir pour trouver tous les chunks complets.
    Un chunk est valide si le dossier mixed_1B_chunk_X/chunk/ existe
    et contient au moins un .pt.
    """
    available = []
    if not os.path.exists(data_dir):
        return available

    for entry in sorted(os.listdir(data_dir)):
        if not entry.startswith('mixed_1B_chunk_'):
            continue
        chunk_subdir = os.path.join(data_dir, entry, 'chunk')
        if not os.path.isdir(chunk_subdir):
            continue
        pt_files = sorted([f for f in os.listdir(chunk_subdir) if f.endswith('.pt')])
        if len(pt_files) > 0:
            # Extraire l'ID numÃ©rique
            try:
                chunk_id = int(entry.replace('mixed_1B_chunk_', ''))
                available.append({
                    'id': chunk_id,
                    'dir': chunk_subdir,
                    'files': pt_files,
                })
            except ValueError:
                continue

    # Sort par ID
    available.sort(key=lambda x: x['id'])
    return available

print(f"\nğŸ” Scan des chunks dans {CONFIG['data_dir']}...")
AVAILABLE_CHUNKS = scan_available_chunks(CONFIG['data_dir'])

if args.total_chunks is not None:
    # Override : limiter au nombre demandÃ©
    AVAILABLE_CHUNKS = AVAILABLE_CHUNKS[:args.total_chunks]

NUM_CHUNKS = len(AVAILABLE_CHUNKS)

print(f"   âœ… {NUM_CHUNKS} chunks trouvÃ©s")
if NUM_CHUNKS > 0:
    total_estimated_tokens = NUM_CHUNKS * 1e9  # ~1B par chunk (from downloader)
    print(f"   ğŸ“Š Tokens estimÃ©s : {total_estimated_tokens / 1e9:.0f}B")
    print(f"   ğŸ“‚ Premier chunk : {AVAILABLE_CHUNKS[0]['dir']}")
    print(f"   ğŸ“‚ Dernier chunk  : {AVAILABLE_CHUNKS[-1]['dir']}")
    print(f"   ğŸ“ Fichiers par chunk : {AVAILABLE_CHUNKS[0]['files']}")

if args.dry_run:
    print("\nğŸ“‹ DRY RUN â€” DÃ©tail des chunks :")
    total_size = 0
    for chunk in AVAILABLE_CHUNKS:
        size = sum(
            os.path.getsize(os.path.join(chunk['dir'], f))
            for f in chunk['files']
        )
        total_size += size
        print(f"   chunk_{chunk['id']:03d} : {len(chunk['files'])} fichiers, {size/1e6:.1f} MB")
    print(f"\n   Total disque : {total_size/1e9:.2f} GB")
    print(f"   Total chunks : {NUM_CHUNKS}")
    print("\nâœ… Dry run terminÃ©. Relancer sans --dry-run pour train.")
    sys.exit(0)

if NUM_CHUNKS == 0:
    print("\nâŒ Aucun chunk trouvÃ© ! Lance d'abord le downloader.")
    sys.exit(1)

# ============================================
# SETUP
# ============================================
print(f"\nâœ… Device : {device}")
if device == 'cuda':
    print(f"   GPU  : {torch.cuda.get_device_name(0)}")
    print(f"   VRAM : {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
    # VÃ©rifie BF16 support
    if torch.cuda.is_bf16_supported():
        print(f"   BF16 : âœ… SupportÃ©")
    else:
        print(f"   BF16 : âš ï¸  Non supportÃ© â€” fallback FP16")

tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
tokenizer.add_special_tokens({
    'additional_special_tokens': list(SPECIAL_TOKENS.keys())
})
tokenizer.pad_token = tokenizer.eos_token

# ============================================
# CALCUL STEPS TOTAUX (pour WSD)
# ============================================
# On estime les steps par chunk :
#   tokens_per_chunk â‰ˆ 1B (from downloader)
#   samples = tokens / (seq_len + 1)
#   batches = samples / batch_size
#   steps   = batches / gradient_accumulation
TOKENS_PER_CHUNK_EST = 1_000_000_000
samples_per_chunk = TOKENS_PER_CHUNK_EST // (CONFIG['max_seq_len'] + 1)
batches_per_chunk = samples_per_chunk // CONFIG['batch_size']
steps_per_chunk   = batches_per_chunk // CONFIG['gradient_accumulation']
TOTAL_STEPS       = steps_per_chunk * NUM_CHUNKS

print(f"\nğŸ“ˆ TRAINING PLAN :")
print(f"   Chunks           : {NUM_CHUNKS}")
print(f"   Steps/chunk      : {steps_per_chunk:,}")
print(f"   Total steps      : {TOTAL_STEPS:,}")
print(f"   Tokens totaux    : {NUM_CHUNKS * TOKENS_PER_CHUNK_EST / 1e9:.0f}B")

# ============================================
# WSD SCHEDULER â€” DYNAMIQUE
# ============================================
class WSDScheduler:
    """
    Warmup â€“ Stable â€“ Decay
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    InspirÃ© Qwen/LLaMA best practices :
    â€¢ Warmup trÃ¨s court (2%) : convergence rapide du dÃ©but
    â€¢ Stable trÃ¨s long (90%) : le modÃ¨le apprend sans perturbation
    â€¢ Decay Ã  la fin (8%)    : affine les poids pour la meilleure qualitÃ©

    KEY POINT : total_steps est calculÃ© Ã  partir du nombre de chunks
    dÃ©tectÃ©s. Si tu rajoutes des chunks (le downloader continue),
    relancez le script â†’ il recalcule total_steps, le decay se pousse
    automatiquement vers la fin. Le checkpoint garde current_step,
    donc la reprise est seamless.
    """
    def __init__(self, optimizer, max_lr, total_steps,
                 warmup_ratio=0.02, decay_ratio=0.08, min_lr_ratio=0.1):
        self.optimizer   = optimizer
        self.max_lr      = max_lr
        self.min_lr      = max_lr * min_lr_ratio
        self.total_steps = total_steps

        self.warmup_steps = int(total_steps * warmup_ratio)
        self.decay_steps  = int(total_steps * decay_ratio)
        self.stable_steps = total_steps - self.warmup_steps - self.decay_steps

        self.current_step = 0

        print(f"\nğŸ“ˆ WSD LR SCHEDULE :")
        print(f"   â”œâ”€ Warmup  : {self.warmup_steps:>8,} steps  ({warmup_ratio*100:>4.1f}%)")
        print(f"   â”œâ”€ Stable  : {self.stable_steps:>8,} steps  ({self.stable_steps/total_steps*100:>4.1f}%)")
        print(f"   â”œâ”€ Decay   : {self.decay_steps:>8,} steps  ({decay_ratio*100:>4.1f}%)")
        print(f"   â””â”€ Total   : {self.total_steps:>8,} steps")
        print(f"   LR : {self.min_lr:.2e} â†’ {self.max_lr:.2e}")

    def get_lr(self):
        step = self.current_step

        if step < self.warmup_steps:
            # Phase 1 : Warmup linÃ©aire
            return self.max_lr * (step / max(self.warmup_steps, 1))

        elif step < self.warmup_steps + self.stable_steps:
            # Phase 2 : Stable â€” LR constant Ã  max_lr
            return self.max_lr

        else:
            # Phase 3 : Decay cosine vers min_lr
            decay_step = step - self.warmup_steps - self.stable_steps
            progress   = min(decay_step / max(self.decay_steps, 1), 1.0)
            cosine     = 0.5 * (1.0 + math.cos(math.pi * progress))
            return self.min_lr + (self.max_lr - self.min_lr) * cosine

    def step(self):
        self.current_step += 1
        lr = self.get_lr()
        for pg in self.optimizer.param_groups:
            pg['lr'] = lr
        return lr

    def get_last_lr(self):
        return [self.get_lr()]

    def state_dict(self):
        return {'current_step': self.current_step}

    def load_state_dict(self, sd):
        self.current_step = sd['current_step']

# ============================================
# LAZY CHUNK DATASET
# ============================================
class LazyChunkDataset(Dataset):
    """
    Charge UN chunk en RAM.
    Un chunk = plusieurs .pt (un par dataset source).
    On les concatÃ¨ne en sÃ©quence.
    """
    def __init__(self, chunk_info, seq_len, pad_token_id):
        self.seq_len     = seq_len
        self.pad_token_id = pad_token_id
        self.tokens      = None
        self.num_samples = 0
        self._load(chunk_info)

    def _load(self, chunk_info):
        print(f"   ğŸ“¥ Loading chunk_{chunk_info['id']:03d} ({len(chunk_info['files'])} fichiers)...")
        t0 = time.time()

        all_tokens = []
        for fname in chunk_info['files']:
            fpath = os.path.join(chunk_info['dir'], fname)
            try:
                data = torch.load(fpath, map_location='cpu')
                # Le downloader sauvegarde des listes ou des tensors
                if isinstance(data, list):
                    all_tokens.extend(data)
                elif isinstance(data, torch.Tensor):
                    all_tokens.extend(data.tolist())
                else:
                    print(f"      âš ï¸  {fname} : type inconnu ({type(data)}), skip")
            except Exception as e:
                print(f"      âš ï¸  {fname} : erreur load ({e}), skip")
                continue

        if len(all_tokens) == 0:
            raise ValueError(f"Chunk {chunk_info['id']} : aucun token chargÃ© !")

        self.tokens     = torch.tensor(all_tokens, dtype=torch.long)
        self.num_samples = len(self.tokens) // (self.seq_len + 1)

        elapsed = time.time() - t0
        print(f"   âœ… {len(self.tokens)/1e6:.1f}M tokens â†’ {self.num_samples:,} samples ({elapsed:.1f}s)")

    def __len__(self):
        return self.num_samples

    def __getitem__(self, idx):
        start = idx * (self.seq_len + 1)
        end   = start + self.seq_len + 1
        chunk = self.tokens[start:end]

        if len(chunk) < self.seq_len + 1:
            pad_len = self.seq_len + 1 - len(chunk)
            chunk = torch.cat([
                chunk,
                torch.full((pad_len,), self.pad_token_id, dtype=torch.long)
            ])

        return chunk[:-1], chunk[1:]

    def unload(self):
        del self.tokens
        self.tokens = None
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

# ============================================
# CHECKPOINT MANAGER
# ============================================
class CheckpointManager:
    def __init__(self, path):
        self.path = path
        os.makedirs(os.path.dirname(path), exist_ok=True)

    def save(self, model, optimizer, scheduler, metadata):
        m = model._orig_mod if hasattr(model, '_orig_mod') else model
        checkpoint = {
            'model_state_dict':     m.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'scheduler_state_dict': scheduler.state_dict(),
            'global_step':          metadata['global_step'],
            'next_chunk_idx':       metadata['next_chunk_idx'],
            'training_history':     metadata['training_history'],
            'total_training_time':  metadata.get('total_training_time', 0),
            'config':               CONFIG,
            'last_save':            datetime.now().isoformat(),
        }
        tmp = self.path + '.tmp'
        torch.save(checkpoint, tmp)
        os.replace(tmp, self.path)
        print(f"      ğŸ’¾ Checkpoint â†’ {self.path}")

    def load(self):
        if not os.path.exists(self.path):
            return None
        print(f"\nğŸ“‚ Checkpoint trouvÃ© : {self.path}")
        cp = torch.load(self.path, map_location='cpu')
        print(f"   âœ… Step          : {cp['global_step']:,}")
        print(f"   âœ… Next chunk    : {cp['next_chunk_idx']}")
        print(f"   âœ… Temps total   : {cp.get('total_training_time', 0)/3600:.2f}h")
        return cp

# ============================================
# VALIDATION
# ============================================
@torch.no_grad()
def validate(model, val_loader, device, max_batches=50):
    model.eval()
    total_loss   = 0
    total_tokens = 0

    for i, (x, y) in enumerate(val_loader):
        if i >= max_batches:
            break
        x = x.to(device)
        y = y.to(device)
        with torch.amp.autocast('cuda', dtype=torch.bfloat16):
            _, loss = model(x, targets=y)
        mask = (y != tokenizer.pad_token_id)
        total_loss   += loss.item() * mask.sum().item()
        total_tokens += mask.sum().item()

    avg_loss    = total_loss / max(total_tokens, 1)
    perplexity  = math.exp(min(avg_loss, 10))
    model.train()
    return perplexity, avg_loss

# ============================================
# TRAIN ONE EPOCH = ONE CHUNK
# ============================================
def train_one_chunk(
    model, chunk_info, optimizer, scheduler,
    val_loader, checkpoint_manager, training_history,
    global_step, total_training_time, chunk_idx
):
    epoch_num = chunk_idx + 1  # Display 1-indexed

    print(f"\n{'=' * 80}")
    print(f"ğŸ“¦ EPOCH {epoch_num}/{NUM_CHUNKS}  â€”  chunk_{chunk_info['id']:03d}")
    print(f"   LR actuel : {scheduler.get_last_lr()[0]:.2e}")
    print(f"{'=' * 80}")

    # --- Load chunk ---
    try:
        train_dataset = LazyChunkDataset(
            chunk_info, CONFIG['max_seq_len'], tokenizer.pad_token_id
        )
    except Exception as e:
        print(f"   âŒ Erreur load chunk : {e}")
        return global_step, total_training_time

    train_loader = DataLoader(
        train_dataset,
        batch_size=CONFIG['batch_size'],
        shuffle=True,
        num_workers=4,
        pin_memory=True,
        persistent_workers=True,
        drop_last=True,
    )

    num_batches = len(train_loader)
    print(f"   ğŸ“Š {num_batches:,} batches")

    model.train()
    epoch_loss     = 0.0
    valid_batches  = 0
    t_start        = time.time()

    pbar = tqdm(
        train_loader,
        desc=f"Epoch {epoch_num}/{NUM_CHUNKS}",
        leave=True,
    )

    for batch_idx, (x, y) in enumerate(pbar):
        try:
            x = x.to(device)
            y = y.to(device)

            # BF16 autocast
            with torch.amp.autocast('cuda', dtype=torch.bfloat16):
                logits, loss = model(x, targets=y)
                loss = loss / CONFIG['gradient_accumulation']

            if torch.isnan(loss) or torch.isinf(loss):
                optimizer.zero_grad(set_to_none=True)
                continue

            loss.backward()

            if (batch_idx + 1) % CONFIG['gradient_accumulation'] == 0:
                torch.nn.utils.clip_grad_norm_(
                    model.parameters(), CONFIG['max_grad_norm']
                )
                optimizer.step()
                optimizer.zero_grad(set_to_none=True)
                scheduler.step()

                global_step += 1

                # --- Validation pÃ©riodique ---
                if global_step % CONFIG['validate_every_steps'] == 0 and val_loader is not None:
                    val_ppl, val_loss = validate(
                        model, val_loader, device, CONFIG['val_batches']
                    )
                    print(f"\n      {'â”€' * 65}")
                    print(f"      ğŸ“Š Step {global_step:,} | PPL {val_ppl:7.2f} | "
                          f"Val Loss {val_loss:.4f} | LR {scheduler.get_last_lr()[0]:.2e}")
                    print(f"      {'â”€' * 65}\n")

                    training_history['validations'].append({
                        'step':       global_step,
                        'epoch':      epoch_num,
                        'chunk_id':   chunk_info['id'],
                        'perplexity': val_ppl,
                        'val_loss':   val_loss,
                        'train_loss': loss.item() * CONFIG['gradient_accumulation'],
                        'lr':         scheduler.get_last_lr()[0],
                    })

            epoch_loss    += loss.item() * CONFIG['gradient_accumulation']
            valid_batches += 1

            if batch_idx % 20 == 0:
                pbar.set_postfix({
                    'loss': f'{loss.item() * CONFIG["gradient_accumulation"]:.4f}',
                    'lr':   f'{scheduler.get_last_lr()[0]:.2e}',
                    'step': f'{global_step:,}',
                })

        except RuntimeError as e:
            if "out of memory" in str(e):
                print(f"\n      âŒ OOM au batch {batch_idx} â€” cleanup...")
                torch.cuda.empty_cache()
                optimizer.zero_grad(set_to_none=True)
                gc.collect()
                continue
            raise

    pbar.close()

    # --- Fin epoch ---
    avg_loss = epoch_loss / max(valid_batches, 1)

    # Val finale de l'epoch
    val_ppl, val_loss = (None, None)
    if val_loader is not None:
        val_ppl, val_loss = validate(model, val_loader, device, CONFIG['val_batches'])

    epoch_time = time.time() - t_start
    total_training_time += epoch_time

    print(f"\n   {'â”€' * 70}")
    print(f"   âœ… EPOCH {epoch_num} TERMINÃ‰E")
    print(f"      Train Loss : {avg_loss:.4f}")
    if val_ppl is not None:
        print(f"      Val PPL    : {val_ppl:.2f}")
        print(f"      Val Loss   : {val_loss:.4f}")
    print(f"      Temps      : {epoch_time / 60:.1f} min")
    print(f"      LR         : {scheduler.get_last_lr()[0]:.2e}")
    print(f"   {'â”€' * 70}")

    training_history['epochs'].append({
        'epoch':      epoch_num,
        'chunk_id':   chunk_info['id'],
        'train_loss': avg_loss,
        'val_loss':   val_loss,
        'val_ppl':    val_ppl,
        'global_step': global_step,
        'lr':         scheduler.get_last_lr()[0],
        'time_s':     epoch_time,
    })

    # --- Checkpoint ---
    if epoch_num % CONFIG['save_every_epochs'] == 0:
        checkpoint_manager.save(
            model, optimizer, scheduler,
            metadata={
                'global_step':         global_step,
                'next_chunk_idx':      chunk_idx + 1,
                'training_history':    training_history,
                'total_training_time': total_training_time,
            }
        )

    # --- Cleanup chunk de la RAM ---
    train_dataset.unload()
    del train_loader, train_dataset
    gc.collect()
    torch.cuda.empty_cache()

    return global_step, total_training_time

# ============================================
# MAIN
# ============================================
def main():
    from HessGpt import HessGPT

    print("\n" + "=" * 80)
    print("ğŸ¤– CRÃ‰ATION DU MODÃˆLE")
    print("=" * 80)

    if device == 'cpu':
        print("\nâš ï¸  GPU fortement recommandÃ©e pour le training !")

    checkpoint_manager = CheckpointManager(CONFIG['checkpoint_file'])

    # --- ModÃ¨le ---
    print(f"\nğŸ—ï¸  HessGPT ({CONFIG['embed_dim']}d, {CONFIG['num_layers']}L, {CONFIG['num_heads']}h)...")
    model = HessGPT(
        vocab_size=CONFIG['vocab_size'],
        embed_dim=CONFIG['embed_dim'],
        num_heads=CONFIG['num_heads'],
        num_layers=CONFIG['num_layers'],
        max_seq_len=CONFIG['max_seq_len'],
        dropout=CONFIG['dropout'],
        use_rope=CONFIG['use_rope'],  # âœ¨ RoPE activÃ©
    ).to(device)
    # âš¡ MIXED PRECISION â€” STRATÃ‰GIE CORRECTE :
    # - Poids du modÃ¨le : FP32 (pour optimizer states prÃ©cis)
    # - Forward/Backward : BF16 via autocast (rapide sur H100)
    # - Optimizer states : FP32 automatiquement (PyTorch cast les grads)
    # âš ï¸  NE PAS faire model.to(bfloat16) â€” Ã§a met les optimizer states
    #     en BF16 aussi â†’ les petits updates (LRÃ—grad â‰ˆ 3e-6) sont
    #     arrondis Ã  0 par la prÃ©cision BF16 â†’ le modÃ¨le n'apprend plus.
    # Sur H100 avec 0.5B : poids FP32 + optimizer = ~6GB sur 80GB â†’ OK
    print(f"   âœ… Poids en FP32 (optimizer states prÃ©cis)")
    print(f"   âœ… Forward/Backward via autocast BF16 (rapide sur H100)")

    total_params = sum(p.numel() for p in model.parameters())
    print(f"   âœ… ParamÃ¨tres : {total_params / 1e6:.1f}M")
    
    # Afficher les dÃ©tails de l'architecture avec RoPE
    if hasattr(model, 'count_parameters'):
        params_detail = model.count_parameters()
        print(f"\nğŸ“Š DÃ©tails de l'architecture :")
        print(f"   â€¢ Token embeddings     : {params_detail['token_embeddings'] / 1e6:.1f}M")
        print(f"   â€¢ Position embeddings  : {params_detail['position_embeddings'] / 1e6:.1f}M", end="")
        if CONFIG['use_rope']:
            print(f" âœ¨ (RoPE = 0 params!)")
        else:
            print()
        print(f"   â€¢ Transformer blocks   : {params_detail['transformer_blocks'] / 1e6:.1f}M")
        print(f"   â€¢ Final LayerNorm      : {params_detail['final_ln'] / 1e3:.1f}K")
        print(f"   â€¢ Output head          : {params_detail['output_head'] / 1e6:.1f}M (partagÃ©)")
        if CONFIG['use_rope']:
            saved_params = params_detail['token_embeddings'] + params_detail['position_embeddings']
            # Position embeddings classiques = vocab_size * embed_dim
            classic_pos_emb = CONFIG['max_seq_len'] * CONFIG['embed_dim']
            print(f"\n   ğŸ’° Ã‰conomie RoPE : ~{classic_pos_emb / 1e6:.1f}M paramÃ¨tres")


    # --- Compile ---
    if CONFIG['use_compile'] and device == 'cuda':
        print(f"\nâš¡ torch.compile (mode={CONFIG['compile_mode']})...")
        try:
            model = torch.compile(model, mode=CONFIG['compile_mode'])
            print(f"   âœ… CompilÃ©")
        except Exception as e:
            print(f"   âš ï¸  Compilation Ã©chouÃ©e : {e}")

    # --- Optimizer ---
    # AdamW en FP32 pour la stabilitÃ© (mÃªme si modÃ¨le en BF16)
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=CONFIG['learning_rate'],
        betas=(0.9, 0.95),
        weight_decay=0.1,
        fused=(device == 'cuda'),
    )

    # --- Scheduler WSD ---
    scheduler = WSDScheduler(
        optimizer,
        max_lr=CONFIG['learning_rate'],
        total_steps=TOTAL_STEPS,
        warmup_ratio=CONFIG['warmup_ratio'],
        decay_ratio=CONFIG['decay_ratio'],
        min_lr_ratio=CONFIG['min_lr_ratio'],
    )

    # --- Training history ---
    training_history = {
        'config':          CONFIG,
        'special_tokens':  SPECIAL_TOKENS,
        'total_params':    total_params,
        'num_chunks':      NUM_CHUNKS,
        'total_steps':     TOTAL_STEPS,
        'epochs':          [],
        'validations':     [],
        'start_time':      datetime.now().isoformat(),
    }

    global_step          = 0
    start_chunk_idx      = 0
    total_training_time  = 0

    # --- Resume from checkpoint ---
    checkpoint = checkpoint_manager.load()
    if checkpoint:
        print("\nâ™»ï¸  REPRISE DU TRAINING")
        unwrapped = model._orig_mod if hasattr(model, '_orig_mod') else model
        unwrapped.load_state_dict(checkpoint['model_state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])

        global_step         = checkpoint['global_step']
        start_chunk_idx     = checkpoint['next_chunk_idx']
        training_history    = checkpoint['training_history']
        total_training_time = checkpoint.get('total_training_time', 0)
        print(f"   â–¶ï¸  Reprise Ã  chunk index {start_chunk_idx} (step {global_step:,})")

    # --- Validation loader (chunk 0) ---
    print(f"\nğŸ“¥ PrÃ©paration validation (chunk 0)...")
    val_loader = None
    if len(AVAILABLE_CHUNKS) > 0:
        val_chunk = AVAILABLE_CHUNKS[0]
        try:
            val_dataset = LazyChunkDataset(
                val_chunk, CONFIG['max_seq_len'], tokenizer.pad_token_id
            )
            # On prend seulement val_ratio des samples pour val
            val_size = max(1, int(len(val_dataset) * CONFIG['val_ratio']))
            val_subset = torch.utils.data.Subset(val_dataset, list(range(val_size)))
            val_loader = DataLoader(
                val_subset,
                batch_size=CONFIG['batch_size'],
                num_workers=2,
                pin_memory=True,
            )
            print(f"   âœ… Val set : {val_size:,} samples")
        except Exception as e:
            print(f"   âš ï¸  Val loader Ã©chouÃ© : {e}")
            val_loader = None

    # ============================================
    # TRAINING LOOP
    # ============================================
    print("\n" + "=" * 80)
    print("ğŸš€ DÃ‰MARRAGE TRAINING")
    print(f"   Chunks : {start_chunk_idx} â†’ {NUM_CHUNKS}")
    print(f"   Total tokens : {NUM_CHUNKS * 1e9 / 1e9:.0f}B")
    print("=" * 80)

    overall_start = time.time()

    for chunk_idx in range(start_chunk_idx, NUM_CHUNKS):
        chunk_info = AVAILABLE_CHUNKS[chunk_idx]

        # Skip chunk 0 en training (utilisÃ© pour val) sauf si c'est le seul
        # En fait on train dessus aussi, la val n'utilise qu'un petit subset
        # donc pas de conflit.

        try:
            global_step, total_training_time = train_one_chunk(
                model=model,
                chunk_info=chunk_info,
                optimizer=optimizer,
                scheduler=scheduler,
                val_loader=val_loader,
                checkpoint_manager=checkpoint_manager,
                training_history=training_history,
                global_step=global_step,
                total_training_time=total_training_time,
                chunk_idx=chunk_idx,
            )
        except KeyboardInterrupt:
            print("\n\nâš ï¸  CTRL+C â€” Sauvegarde d'urgence...")
            checkpoint_manager.save(
                model, optimizer, scheduler,
                metadata={
                    'global_step':         global_step,
                    'next_chunk_idx':      chunk_idx,  # Reprendre CE chunk
                    'training_history':    training_history,
                    'total_training_time': total_training_time,
                }
            )
            print("   âœ… Checkpoint sauvegardÃ©. Relancer pour reprendre.")
            return
        except Exception as e:
            print(f"\nâŒ ERREUR Ã  l'epoch {chunk_idx + 1} :")
            print(traceback.format_exc())
            print("\nğŸ’¾ Checkpoint d'urgence...")
            checkpoint_manager.save(
                model, optimizer, scheduler,
                metadata={
                    'global_step':         global_step,
                    'next_chunk_idx':      chunk_idx,
                    'training_history':    training_history,
                    'total_training_time': total_training_time,
                }
            )
            raise

    # --- FIN ---
    overall_time = time.time() - overall_start

    # Checkpoint final
    checkpoint_manager.save(
        model, optimizer, scheduler,
        metadata={
            'global_step':         global_step,
            'next_chunk_idx':      NUM_CHUNKS,
            'training_history':    training_history,
            'total_training_time': total_training_time,
        }
    )

    print("\n" + "=" * 80)
    print("ğŸ‰ TRAINING TERMINÃ‰ !")
    print("=" * 80)
    print(f"\nğŸ“Š RÃ‰SUMATS :")
    print(f"   Epochs complÃ©tÃ©es : {len(training_history['epochs'])}/{NUM_CHUNKS}")
    print(f"   Steps totaux      : {global_step:,}")
    print(f"   Tokens vus        : {NUM_CHUNKS * 1e9 / 1e9:.0f}B")
    print(f"   Temps training    : {total_training_time / 3600:.2f}h")
    print(f"   Temps rÃ©el        : {overall_time / 3600:.2f}h")

    if training_history['validations']:
        last = training_history['validations'][-1]
        print(f"   PPL final         : {last['perplexity']:.2f}")
        print(f"   Loss final        : {last['val_loss']:.4f}")

    print(f"\nğŸ’¾ Checkpoint : {checkpoint_manager.path}")

    # Save history JSON
    history_path = CONFIG['checkpoint_file'].replace('.pt', '_history.json')
    with open(history_path, 'w') as f:
        json.dump(training_history, f, indent=2, default=str)
    print(f"ğŸ“ History     : {history_path}")
    print("\nâœ… DONE !")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  Interrompu â€” checkpoint sauvegardÃ©, relancer pour continuer.")
    except Exception as e:
        print(f"\n\nâŒ ERREUR FATALE :")
        print(traceback.format_exc())
    finally:
        print("\nğŸ‘‹ Fin du script")
