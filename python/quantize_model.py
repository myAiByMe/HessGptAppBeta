#!/usr/bin/env python3
"""
Script pour quantifier le mod√®le HessGPT en INT8
R√©duit la taille du mod√®le de ~75% et acc√©l√®re l'inf√©rence

Usage:
    python quantize_model.py --input model.ptl --output model_quantized.ptl
"""

import argparse
import torch
import os
from torch.quantization import quantize_dynamic


def quantize_model(input_path, output_path, quantization_type='dynamic'):
    """
    Quantifie le mod√®le en INT8
    
    Args:
        input_path: Chemin vers le mod√®le TorchScript (.ptl)
        output_path: Chemin de sortie pour le mod√®le quantifi√©
        quantization_type: Type de quantification ('dynamic' ou 'static')
    """
    
    print(f"üì• Chargement du mod√®le depuis {input_path}")
    model = torch.jit.load(input_path)
    
    original_size = os.path.getsize(input_path) / (1024 * 1024)
    print(f"   Taille originale: {original_size:.2f} MB")
    
    print(f"\n‚öôÔ∏è  Quantification {quantization_type} en cours...")
    
    if quantization_type == 'dynamic':
        # Quantification dynamique (recommand√© pour les mod√®les de langage)
        # Quantifie les poids en INT8, les activations restent en FP32
        quantized_model = torch.quantization.quantize_dynamic(
            model,
            qconfig_spec={torch.nn.Linear},  # Quantifier les couches lin√©aires
            dtype=torch.qint8
        )
    else:
        # Pour la quantification statique, il faudrait calibrer avec des donn√©es
        raise NotImplementedError("La quantification statique n√©cessite une calibration")
    
    print("üíæ Sauvegarde du mod√®le quantifi√©...")
    quantized_model.save(output_path)
    
    quantized_size = os.path.getsize(output_path) / (1024 * 1024)
    reduction = ((original_size - quantized_size) / original_size) * 100
    
    print(f"\n‚úÖ Quantification termin√©e !")
    print(f"   Taille originale:  {original_size:.2f} MB")
    print(f"   Taille quantifi√©e: {quantized_size:.2f} MB")
    print(f"   R√©duction:         {reduction:.1f}%")
    
    # Test rapide
    print("\nüß™ Test du mod√®le quantifi√©...")
    test_input = torch.randint(0, 50257, (1, 128))
    
    try:
        with torch.no_grad():
            output = quantized_model(test_input)
        print(f"   Shape de sortie: {output.shape}")
        print("   ‚úÖ Le mod√®le quantifi√© fonctionne correctement!")
    except Exception as e:
        print(f"   ‚ö†Ô∏è  Erreur lors du test: {e}")
    
    return output_path


def compare_models(original_path, quantized_path, num_tests=10):
    """
    Compare les performances et la pr√©cision entre les mod√®les
    """
    print("\nüìä Comparaison des mod√®les...")
    
    original = torch.jit.load(original_path)
    quantized = torch.jit.load(quantized_path)
    
    import time
    
    # Test de latence
    test_input = torch.randint(0, 50257, (1, 128))
    
    # Warm-up
    with torch.no_grad():
        _ = original(test_input)
        _ = quantized(test_input)
    
    # Benchmark original
    start = time.time()
    for _ in range(num_tests):
        with torch.no_grad():
            _ = original(test_input)
    original_time = (time.time() - start) / num_tests * 1000
    
    # Benchmark quantifi√©
    start = time.time()
    for _ in range(num_tests):
        with torch.no_grad():
            _ = quantized(test_input)
    quantized_time = (time.time() - start) / num_tests * 1000
    
    speedup = original_time / quantized_time
    
    print(f"\n‚è±Ô∏è  Latence moyenne (sur {num_tests} tests):")
    print(f"   Original:   {original_time:.2f} ms")
    print(f"   Quantifi√©:  {quantized_time:.2f} ms")
    print(f"   Speedup:    {speedup:.2f}x")
    
    # Test de pr√©cision (diff√©rence de sortie)
    with torch.no_grad():
        out_original = original(test_input)
        out_quantized = quantized(test_input)
    
    max_diff = torch.max(torch.abs(out_original - out_quantized)).item()
    mean_diff = torch.mean(torch.abs(out_original - out_quantized)).item()
    
    print(f"\nüìè Diff√©rence de sortie:")
    print(f"   Diff√©rence max:     {max_diff:.6f}")
    print(f"   Diff√©rence moyenne: {mean_diff:.6f}")


def main():
    parser = argparse.ArgumentParser(
        description="Quantifier HessGPT pour optimiser la taille et la vitesse"
    )
    parser.add_argument(
        "--input",
        type=str,
        required=True,
        help="Chemin vers le mod√®le TorchScript (.ptl)"
    )
    parser.add_argument(
        "--output",
        type=str,
        default="model_quantized.ptl",
        help="Chemin de sortie pour le mod√®le quantifi√©"
    )
    parser.add_argument(
        "--type",
        type=str,
        default="dynamic",
        choices=["dynamic", "static"],
        help="Type de quantification"
    )
    parser.add_argument(
        "--compare",
        action="store_true",
        help="Comparer les performances avec le mod√®le original"
    )
    
    args = parser.parse_args()
    
    if not os.path.exists(args.input):
        print(f"‚ùå Erreur: Le mod√®le {args.input} n'existe pas!")
        return
    
    quantize_model(
        input_path=args.input,
        output_path=args.output,
        quantization_type=args.type
    )
    
    if args.compare:
        compare_models(args.input, args.output)


if __name__ == "__main__":
    main()
